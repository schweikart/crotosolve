\chapter{Evaluation}
\label{chap:evaluation}

To evaluate the quality of the algorithm proposed in chapter
\ref{chap:gradient-free}, I have implemented a proof-of-concept optimizer.
I will compare \texttt{Crotosolve} to several other optimizers frequently used
in QML by creating a loss curve benchmark for all optimizers.
The benchmark features parameterizable quantum circuits with various degrees of
expressibility and from various research applications.

\section{Proof of Concept}
\label{sec:proof-of-concept}
To put the approach proposed in chapter \ref{chap:gradient-free} to test,
I have implemented a proof-of-concept application with PennyLane
\cite{bergholm_pennylane_2018}.
PennyLane is a popular quantum computing SDK with extensive documentation and a
library of readily implemented optimizers \cite{unitary_fund_team_results_2022}.
Particularly, PennyLane is currently the only quantum SDK to implement the
\texttt{Rotosolve} optimizer, to which I want to compare my \texttt{Crotosolve}
optimizer in section \ref{sec:optimizer-comparison}.

Like the \texttt{Rotosolve} algorithm, \texttt{Crotosolve} optimizes each
parameter individually.
For each parameter corresponding to a controlled rotational Pauli gate,
\texttt{Crotosolve} first reconstructs the univariate cost function (see section
\ref{sec:constants}) and then uses a numerical optimizer to find the minimizing
parameter value (see section \ref{sec:minimization}).
Each parameter corresponding to an uncontrolled rotational Pauli gate can be
optimized following the exact approach presented by Ostaszewski et al.
\cite{ostaszewski_structure_2021}.
The pseudocode for this algorithm is stated in algorithm \ref{alg:crotosolve}.

\begin{algorithm}
    \caption{The \texttt{Crotosolve} algorithm updates parameters individually}
    \label{alg:crotosolve}
    %
    \SetKwFunction{ReconstructCrp}{ReconstructCrp}
    \SetKwFunction{RotosolveUpdate}{RotosolveUpdate}
    \SetKwFunction{MakeUnivariate}{MakeUnivariate}
    %
    \KwData{$prev\_params \in \mathbb R^n$, $circuit \in \mathrm{QNode}$}
    \KwResult{$params \in \mathbb R^n$}
    \BlankLine
    $params \gets copy(prev\_params)$\;
    \For{$p \gets 1$ \KwTo $n$}{
        $uni \gets $ \MakeUnivariate{$circuit$, $params$, $p$}\;
        \eIf{$p$ belongs to controlled gate}{
            $recon \gets $ \ReconstructCrp{$uni$} \Comment*[r]{see section \ref{sec:constants}}
            $params[p] \gets \underset{x \in [0, 4\pi]}{\operatorname{argmin}}\, recon(x)$ \Comment*[r]{using numerical minimizer}
        }{
            $params[p] \gets $ \RotosolveUpdate{$uni$}\;
        }
    }
\end{algorithm}

This algorithm allows the optimization of parameterized quantum circuits where
all parameterized gates are either RP or CRP gates.
Furthermore, it is assumed that all gate parameters are used without
preprocessing (e.g., having a $RP(x^2)$ gate for a parameter $x$) in only a
single gate.
The algorithm needs to know which parameters belong to CRP and which belong to
RP gates.
Technically, the optimization approach presented for CRP gates could also be
used for RP gates since a CRP gate's loss function structure is a generalization
of an RP gate's loss function structure.
However, using the CRP optimization approach for RP gates would double the
number of circuit evaluations required to reconstruct its loss function.
While it is trivial to see if a parameter is used in a CRP or RP gate on paper,
looking up this relationship in a given PennyLane circuit is non-trivial,
although certainly possible.
% TODO: really?
Thus, the proof-of-concept implementation always stores the RP and CRP gate
parameters as separate arrays.

\section{Optimizer comparison}
\label{sec:optimizer-comparison}
% TODO: rename, check usages

I will examine \texttt{Crotosolve}'s performance by comparing its loss curve
with the loss curves of other optimizers frequently used in the field of QML.
This comparison includes the Gradient Descent family of optimizers, namely
standard gradient descent with a fixed learning rate, % TODO: cite! and isnt this really stochastic GD?
\texttt{Adam} \cite{kingma_adam_2017} and
\texttt{Adagrad} \cite{duchi_adaptive_2011}.
Additionally, I will test my \texttt{Crotosolve} implementation against
PennyLane's implementation of the \texttt{Rotosolve} algorithm
\cite{ostaszewski_structure_2021,bergholm_pennylane_2018}, which is extended by
Wierichs' paper on ``\emph{\citefield{wierichs_general_2022}{title}}'' to
support further types of gates \cite{wierichs_general_2022}.

To get meaningful results, I chose to run these optimizers on well-known and
widely-used quantum circuits.
Specifically, I use the set of circuit templates presented in a paper on
``Expressibility and Entangling Capability [...]'' metrics by Sukin Sim et al.
\cite{sim_expressibility_2019}.
As shown in this paper, these circuits have various degrees of expressibility
and entangling capability. % TODO why is that good
% TODO mention that this set of circuits has also been used as a benchmark in
%      other cases

The optimizers are compared through loss curves generated from exemplary
circuits.
These loss curves are recorded with respect to the number of circuit
evaluations, even though it would be more common in classical Machine Learning
to use the number of iterations or taken time instead of the evaluations.
The time taken to run these quantum circuits on classical hardware using
state-vector simulation is hardly representative of the actual runtime on
quantum devices since this simulation task is proven to be inefficient.
% TODO: cite!
Additionally, the optimizers benefit to varying degrees from optimizations made
in the simulation.
For example, gradient-based optimizers benefit from PennyLane's gradient
evaluation optimization, which is unavailable on real quantum hardware.
% TODO: cite!
Meanwhile, gradient-free optimizers like \texttt{Crotosolve} and
\texttt{Rotosolve} cannot take advantage of these simulation tricks.
On the other hand, iterations put \texttt{Crotosolve} and \texttt{Rotosolve} in
favor since each of their iterations encompasses many univariate optimizations.
Circuit evaluations, however, are proportional to the execution time on real
quantum hardware as long as the classical work required between evaluations is
negligible.

\begin{figure}
    \centering
    \subfloat[Circuit 11]{
        \includegraphics[width=0.5\textwidth]{loss-curve_sim11_4x3.pdf}
    }
    \subfloat[Circuit 14]{
        \includegraphics[width=0.5\textwidth]{loss-curve_sim14_4x3.pdf}
    }
    \caption{Various optimizers minimize the expectation of two circuits from
        \cite{sim_expressibility_2019} with four qubits and three layers.
        Full lines show the average loss curve.
        Transparent areas show the error, which is the average $\pm$ its
        standard deviation.}
    \label{fig:avg-loss-curve}
\end{figure}

I have generated many random initial parameter values for each circuit to
reduce statistical noise through averaging.
Each pair of circuits and parameter values is run with each optimizer to
record the loss curve of its optimization as a result.
These results are then aggregated in average loss curves for each pair of
circuits and optimizers.

\autoref{fig:avg-loss-curve} shows the average loss curves for each
optimizer on circuit 11.
I have selected this chart as it shows all the key characteristics described in
the following, but the charts for all other evaluated circuits can be found in
appendix \ref{chap:appendix}.
Furthermore, the circuit and initial values data for all evaluated circuits, as
well as the recorded loss curve data is available on GitHub
\cite{schweikart_schweikartcrotosolve_2023}.

% TODO describe the device this has been run on

\subsubsection*{Results}

\texttt{Crotosolve}'s loss curve generally is very similar to
\texttt{Rotosolve}'s loss curve, although consistently below.
For RP gates, this is easy to explain since both algorithms are identical.
For CRP gates, both algorithms use the same strategy, although different methods
to achieve their common goal.
PennyLane's implementation of the \texttt{Rotosolve} algorithm proposed by
Ostaszewski et al. is augmented with the work of Wierichs et al.
\cite{ostaszewski_structure_2021,wierichs_general_2022,bergholm_pennylane_2018}.
Wierichs' work allows the algorithm to reconstruct univariate loss functions
from almost any quantum circuit through a discrete Fourier series transform.
While both \texttt{Crotosolve} and \texttt{Rotosolve} cache an evaluation value
between RP gate optimization steps, only \texttt{Crotosolve} does so for
CRP gates too.
This enhanced caching ensures that \texttt{Crotosolve}'s loss curve is always
slightly better than \texttt{Rotosolve}'s.

\texttt{Crotosolve}'s loss curves are also well below loss curves from
gradient-based optimizers in most instances.
While \texttt{Crotosolve}, \texttt{Rotosolve} and \texttt{Adam} typically
approach the minimum value, $-1$, within the first 250 circuit evaluations,
\texttt{Stochastic Gradient Descent} and \texttt{Adagrad}'s loss curves progress
much slower towards this value.
In direct comparison with \texttt{Adam}, \texttt{Crotosolve}'s loss curve
typically descends much quicker in the beginning and reaches a point close to
the minimum much more quickly.
Still, \texttt{Adam} consistently reaches loss values as low or even lower than
\texttt{Crotosolve} eventually within the shown 250 iterations.
TODO: adam intersects rotosolve.
An exception to this observation is shown by the loss curves of circuit 14.
In this case, TODO.

It is worth noting that the loss curves of both \texttt{Rotosolve} and
\texttt{Crotosolve} have a low variance throughout the optimization process in
all evaluated instances.
This means that the quick descent to the minimum value happens consistently
across the evaluated instances.
On the other hand, the gradient-based optimizers show a much higher variance in
these experiments.
More specifically, \texttt{Stochastic Gradient Descent} and \texttt{Adagrad}
typically demonstrate a standard deviation of more than $0.2$ throughout the
entire optimization process.
Meanwhile, \texttt{Adam} shows a similar standard deviation in the beginning and
quickly progresses towards a standard deviation near zero as it approaches the
$250$ iterations.

\section{Discussion}
\label{sec:evaluation-discussion}

As demonstrated in this evaluation, \texttt{Crotosolve} can outperform
gradient-based methods consistently on the circuits considered here.
However, it should be noted that the gradient-based optimizers have been used
with their default hyperparameter values for this evaluation.
In many cases, achieving good results with Machine Learning optimizers requires
elaborate hyper-parameter tweaking.
While this suggests that these optimizers could potentially yield better results
with hyper-parameter tweaking, it also demonstrates that \texttt{Crotosolve}
produces competitive and consistent results without relying on hyper-parameter
settings.
% TODO: but if Crotosolve is outperformed, there is no way to improve it

While \texttt{Crotosolve} shows slightly better results, PennyLane's
\texttt{Rotosolve} implementation is more general due to Wierichs' work on the
reconstruction of arbitrary univariate loss functions.
This generality enables two use cases that are not covered by \texttt{Crotosolve}.
First, it allows \texttt{Rotosolve} to optimizer parameters used in gate types
not explicitly covered in the optimizer implementation whereas
\texttt{Crotosolve} requires specific implementations for each parameterized
gate type.
% TODO: mention maintainability
Second, it enables \texttt{Rotosolve} to optimize parameters used in multiple
gates.
While this is uncommon in parameterized quantum circuits designed to be used
with one parameter per gate, other variational algorithms like QAOA use
parameters to configure whole groups of gates.
% TODO: check if QAOA needs this. also, this is unclear.
However, this generality comes at the cost of having to compute the frequency
spectrum for each parameter.
This is computationally expensive but still polynomial.
It is sufficient to compute these frequencies once before the first
\texttt{Rotosolve} iteration.
% TODO: do better
