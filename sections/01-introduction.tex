\chapter{Introduction}
\label{chap:intro}

While the first theoretical foundations of quantum computers have already been
developed in the 1980s, quantum computers have only recently gathered widespread
attention with the development and availability of real quantum computing
hardware \cite{nielsen_quantum_2007,hidary_quantum_2021}.
The field is evolving rapidly, with new tools, algorithms, and hardware being
released every month.
% TODO: source?
Still, today's quantum devices are subject to high amounts of noise, have a very
limited number of qubits, and are not fully connected.
These limitations often further reduce the number of available qubits and gates
needed to perform a given task \cite{cerezo_variational_2021}.
Researchers often refer to these limited quantum devices as
\emph{noisy intermediate-scale quantum} (NISQ) devices
\cite{preskill_quantum_2018}.

One promising idea for harnessing the computational power of quantum devices
within the NISQ era is to apply Machine Learning methodology to quantum
computers \cite{cerezo_variational_2021}.
% TODO: citation is for VQAs, not for QML in general!
Machine Learning can often be used to approximate complex functions without much
knowledge about their nature.
% TODO: cite! and is this too vague?
Moreover, Machine Learning can work with or even benefit from a limited amount
of noise \cite{ciliberto_quantum_2018}.

A typical Machine Learning workflow uses a parameterizable function as a
so-called model.
Machine Learning aims to choose parameters for the model so that the model maps
given input data closely to given output data.
In many iterations, an \emph{optimizer} evaluates the model function to compute
the \emph{loss value}, which indicates how much the calculated value deviates
from the correct one.
% TODO: Mention data & labels?
The optimizer uses these evaluations to produce new parameter values and then
repeats its assessment.
This process is known as \emph{training}.
Many state-of-the-art optimizers use an approach called \emph{gradient descent}
to improve the model's parameters.
% TODO: explain gradient descent briefly?
Machine Learning has recently made a tremendous leap due to the availability of
computational resources and improvements in model function design.
% TODO cite!
These improvements allow Machine-Learning applications to train billions of
parameters with terabytes of training data in many iterations.

A \emph{Parameterized Quantum Circuit} (PQC) can replace the classical model
function to transfer the machine-learning methodology to quantum computers.
PQCs are the quantum equivalent of parameterized algorithms like neural
networks.
Gradient-based optimizers can be used for Quantum Machine Learning (QML) too,
but the current price of NISQ devices does not allow for large amounts of data
and large numbers of iterations.
Additionally, each evaluation of a quantum circuit involves multiple shots to
decrease statistical noise introduced by qubit measurement.
% TODO: cite
% TODO: last sentence inconclusive

In ``\emph{\citefield{ostaszewski_structure_2021}{title}},'' Ostaszewski et al.
explore a different optimization approach \cite{ostaszewski_structure_2021}.
They observed that the univariate expected value of a PQC w.r.t. a single
rotational Pauli gate parameter is always sinusoidal.
Using three circuit evaluations with select parameter values, they could
reconstruct this univariate loss function for every parameter value.
Since the curve of sine functions is well-understood, they could then
analytically calculate the parameter value that minimizes this univariate loss
function.
The \texttt{Rotosolve} optimizer uses this approach to optimize each parameter
individually.
While univariate optimization does not guarantee convergence to the global
minimum, experiments show good results with this approach compared to
state-of-the-art optimizers like Adam \cite{kingma_adam_2017}, and SPSA
\cite{spall_multivariate_1992}.
However, the original \texttt{Rotosolve} approach was limited to optimizing
parameters of single-qubit gates.

These promising results naturally pose the question of whether this approach can
be extended to other types of gates.
Because of their similarity with the studied rotational gates, this bachelor's
thesis presents \texttt{Crotosolve}, a similar optimization technique for
controlled rotational Pauli gates.

To develop and investigate the \texttt{Crotosolve} idea, this thesis is
structured as follows.
First, in chapter \ref{chap:background}, I summarize the theoretical background
in quantum computing, Machine Learning, and Quantum Machine Learning required to
understand the idea and proof behind \texttt{Crotosolve}.
Chapter \ref{chap:gradient-free} analyzes the mathematical structure of a
controlled rotational Pauli gate parameter's univariate effect on the
expected value of a quantum circuit measurement.
It also contains a constructive proof for an algorithm that can be used to
determine the prefactors and offsets characterizing the effect function's
specific curve.

The \texttt{Crotosolve} algorithm is put to the test in chapter
\ref{chap:evaluation}.
I present a proof-of-concept implementation in PennyLane, a major quantum
computing SDK \cite{bergholm_pennylane_2022,unitary_fund_team_results_2022}.
Using this implementation and PennyLane's
\texttt{Rotosolve} \cite{ostaszewski_structure_2021},
Adam \cite{kingma_adam_2017},
Adagrad \cite{duchi_adaptive_2011}, and
Stochastic Gradient Descent implementation, I create a benchmark of their loss
curves.
% TODO: cite Stoch. GD?
The data gathered in this benchmark shows that \texttt{Crotosolve} outperforms
the other optimizers in most cases.
Its loss value progresses towards the minimum value much quicker and more
consistently in comparison to its competitors.
However, in cases with high numbers of parameters, the gradient-based Adam
optimizer shows a slightly better final result.
Even in this case, \texttt{Crotosolve} progression towards low loss values is
much steeper. 
A comprehensive discussion of this evaluation can be found in section
\ref{sec:evaluation-discussion}.

While I believe \texttt{Crotosolve} to be the only implementation that exploits
the sinusoidal structure of CRP gate loss functions explicitly, other
gradient-free approaches have been developed in the recent years.
In chapter \ref{chap:related-work}, I outline how contributions by
Wierichs et al. allow the reconstruction of univariate loss functions of almost
arbitrary PQCs \cite{wierichs_general_2022}.
These results have already been implemented in PennyLane's \texttt{Rotosolve}
implementation and were evaluated as part of chapter \ref{chap:evaluation}.

Finally, in chapter \ref{chap:conclusion}, I present a summary of the
contributions from this thesis.
Furthermore, I outline how some of the optimizations of my \texttt{Crotosolve}
implementation and the Wierichs-improved implementation of \texttt{Rotosolve}
could be fused in an optimizer that combines their advantages.
% TODO: conclusion

The proof-of-concept implementation as well as the code and benchmark data of my
evaluation are available on GitHub and Zenodo
\cite{schweikart_schweikartcrotosolve_2023}.
